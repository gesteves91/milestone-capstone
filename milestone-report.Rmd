---
title: "Data Science Capstone Project Milestone Report"
author: "Geanderson Esteves dos Santos"
output: html_document
---

## Executive Summary  

In this report, I describe the results of a prediction algorithm. Thus it comprises an exploratory analysis and goals for the app and algorithm. This document is a high-level description of the results. The report is made available through a Shyny app.

The dataset used is derived from the HC Corpora available at www.corpora.heliohost.org.  The three files used in this project contain blogs, tweets and news content.  


## Initialization

Required Libraries
```{r load_libraries , message=FALSE , warning=FALSE}

library(ggplot2)       # plotting
library(R.utils)       # utility
library(quanteda)      # for tokenization
library(RColorBrewer)  # wordcloud
library(plyr)          # data ordering

```

```{r set_vars , echo = FALSE}

### Set options
options(scipen=999)

### Assign director locations to variables 
twitter_location <- '/home/geanderson/sources/milestone-capstone/en_US.twitter.txt'
news_location <- '/home/geanderson/sources/milestone-capstone/en_US.news.txt'
blogs_location <- '/home/geanderson/sources/milestone-capstone/en_US.blogs.txt'
projDir <- '/home/geanderson/sources/milestone-capstone/'

```
## Descriptive Statistics for Raw Data

A basic summary of the complete file content follows:

File                |  Size on Disk [MB] | Lines   | Word Count [Tokens]
------------------- | ------------------ | ------- | ----------
en_US.twitter.txt   |  `r  file.size(twitter_location)/1000000` | `r countLines(twitter_location)` |  `r unlist(strsplit(system("wc -w /home/geanderson/sources/milestone-capstone/en_US.twitter.txt ", intern = TRUE) , ' '))[2]`
en_US.news.txt      |  `r  file.size(news_location)/1000000`    | `r countLines(news_location)`    |  `r unlist(strsplit(system("wc -w /home/geanderson/sources/milestone-capstone/en_US.news.txt ", intern = TRUE) , ' '))[2]`
en_US.blogs.txt     |  `r  file.size(blogs_location)/1000000`   | `r countLines(blogs_location)`   |  `r unlist(strsplit(system("wc -w /home/geanderson/sources/milestone-capstone/en_US.blogs.txt ", intern = TRUE) , ' '))[2]`


## Data Conditioning
#### Conditioning strategy
Words that contain profanity are not deleted from the corpus as it may contain expressive construct derived from the English language. However, I removed numbers because they are typically unique to a particular circumstance and often not indicative of a future condition.  Since they do not contain any relevant information, I also removed twitter tags.  To enforce consistency,  all text is converted to lower case.

```{r dfm_formation , echo = FALSE , eval=FALSE , message=FALSE , warning=FALSE}
```

#### Create a corpus and document frequency matrix from a sample of each text using the Quanteda library
Here, I randomly sample 1% of each file.  The final app may require a higher sampling rate to obtain a broader spectrum of terms.
```{r sample_files , message=FALSE , warning=FALSE}

sample_fraction <- 0.01

### Twitter
con <- file(twitter_location, "r") 
twitter.list <- as.list(readLines(con , n = -1 , warn = FALSE))
close(con)
twitter.sample <- unlist(sample(twitter.list , sample_fraction*length(twitter.list)))
twitterCorpus_sample <- corpus(twitter.sample)
dfm_twitter_sample <- dfm(twitterCorpus_sample , toLower = TRUE , removeNumbers = TRUE , 
                          removePunct = TRUE , removeTwitter = TRUE , removeSeparators = TRUE)

### News
con <- file(news_location, "r") 
news.list <- readLines(con , n = -1 , warn = FALSE)
close(con)
news.sample <- unlist(sample(news.list  , sample_fraction*length(news.list)))
newsCorpus_sample <- corpus(news.sample)
dfm_news_sample <- dfm(newsCorpus_sample  , toLower = TRUE , removeNumbers = TRUE ,
                       removePunct = TRUE , removeTwitter = TRUE , removeSeparators = TRUE)

### Blogs
con <- file(blogs_location, "r") 
blogs.list <- readLines(con , n = -1 , warn = FALSE)
close(con)
blogs.sample <- unlist(sample(blogs.list , sample_fraction*length(blogs.list)))
blogsCorpus_sample <- corpus(blogs.sample)
dfm_blogs_sample <- dfm(blogsCorpus_sample , toLower = TRUE , removeNumbers = TRUE ,
                        removePunct = TRUE , removeTwitter = TRUE , removeSeparators = TRUE)

```

## Sample Data for Analysis
Create 2-grams and 3-grams from the sampled data with Quanteda
```{r n_grams , message=FALSE , warning=FALSE}

### Twitter n-grams
two_gram_dfm_twitter_sample <- dfm(twitterCorpus_sample , toLower = TRUE , removeNumbers = TRUE ,
                                   removePunct = TRUE , removeTwitter = TRUE , removeSeparators = TRUE , ngram = 2)
three_gram_dfm_twitter_sample <- dfm(twitterCorpus_sample , toLower = TRUE , removeNumbers = TRUE ,
                                     removePunct = TRUE , removeTwitter = TRUE , removeSeparators = TRUE , ngram = 3)

### News n-grams
two_gram_dfm_news_sample <- dfm(newsCorpus_sample , toLower = TRUE , removeNumbers = TRUE , removePunct = TRUE ,
                                removeTwitter = TRUE , removeSeparators = TRUE , ngram = 2)
three_gram_dfm_news_sample <- dfm(newsCorpus_sample , toLower = TRUE , removeNumbers = TRUE , removePunct = TRUE ,
                                  removeTwitter = TRUE , removeSeparators = TRUE , ngram = 3)

### Blogs n-grams
two_gram_dfm_blogs_sample <- dfm(blogsCorpus_sample , toLower = TRUE , removeNumbers = TRUE , removePunct = TRUE ,
                                 removeTwitter = TRUE , removeSeparators = TRUE , ngram = 2)
three_gram_dfm_blogs_sample <- dfm(blogsCorpus_sample , toLower = TRUE , removeNumbers = TRUE , removePunct = TRUE ,
                                   removeTwitter = TRUE , removeSeparators = TRUE , ngram = 3)

```


## Exploratory Analysis of Sample Data:  
#### Analysis of Twitter sample data
```{r twitter_analysis}

par(las=2)
par(mfrow=c(1,3))             # 1 row, 3 col
par( mai = c(.6, 0.55, 0.1, 0.25))

barplot(topfeatures(dfm_twitter_sample, 20) , main = "Top 20 Twitter Terms", ylab = "Twitter Term" , horiz=TRUE , col = 'darkgoldenrod1')
barplot(topfeatures(two_gram_dfm_twitter_sample, 20) , main = "Top 20 Twitter 2-Grams", xlab="Frequency" ,  horiz=TRUE)
barplot(topfeatures(three_gram_dfm_twitter_sample, 20), main = "Top 20 Twitter 3-Grams", horiz=TRUE , col = 'dodgerblue2')

```

#### Analysis of Blogs sample data
```{r blogs_analysis}

par( las=2)
par( mfrow=c(1,3))             # 1 row, 3 col
par( mai = c(.6, 0.55, 0.1, 0.25))

barplot(topfeatures(dfm_blogs_sample, 20) , main = "Top 20 Blog Terms",  ylab = "Term" , horiz=TRUE , col = 'darkgoldenrod1')
barplot(topfeatures(two_gram_dfm_blogs_sample, 20) , main = "Top 20 Blog 2-Grams", xlab="Frequency" ,  horiz=TRUE)
barplot(topfeatures(three_gram_dfm_blogs_sample, 20), main = "Top 20 Blog 3-Grams",  horiz=TRUE , col = 'dodgerblue2')

```

#### Analysis of News sample data
```{r news_analysis}

par(las=2)  #Perpendicular labels
par( mfrow=c(1,3))              # 1 row, 3 col
par( mai = c(.6, 0.55, 0.1, 0.25))

barplot(topfeatures(dfm_news_sample, 20) , main = "Top 20 News Terms",  ylab = "Term" , horiz=TRUE , col = 'darkgoldenrod1')
barplot(topfeatures(two_gram_dfm_news_sample, 20) , main = "Top 20 News 2-Grams", xlab="Frequency"  , horiz=TRUE)
barplot(topfeatures(three_gram_dfm_news_sample, 20), main = "Top 20 News 3-Grams",  horiz=TRUE , col = 'dodgerblue2')

```

#### Comparison of Sampled Files
The below statistics are compiled for the sampled files:

Sampled File   |  Lines Sampled     | Unique Unigrams       | Unique Bigrams | Unique Trigrams
--------------- | ------------------ | --------------------- | -------------- | ---------------
en_US.twitter.txt   |  `r  ndoc(dfm_twitter_sample)` | `r dfm_twitter_sample@Dim[2]` |  `r two_gram_dfm_twitter_sample@Dim[2]` | `r three_gram_dfm_twitter_sample@Dim[2]`
en_US.news.txt   |  `r  ndoc(dfm_news_sample)` | `r dfm_news_sample@Dim[2]` |  `r two_gram_dfm_news_sample@Dim[2]`  |  `r three_gram_dfm_news_sample@Dim[2]`
en_US.blogs.txt   |  `r  ndoc(dfm_blogs_sample)` | `r dfm_blogs_sample@Dim[2]` |  `r two_gram_dfm_blogs_sample@Dim[2]`  | `r three_gram_dfm_blogs_sample@Dim[2]`

## Conclusion
Based on the results, I  concluded that the corpus could be used to generate a distribution of words and n-grams.  It also matches an input phrase and returns a probabilistic response that has some likelihood of predicting the next word in the phrase.  An obvious exception to this is the case where the corpus does not carry the phrase combination.  Another exception is the case where the phrase is a common figure of speech, for example, "on the way to.....".  It is clear that there will be certain corner cases where the simple prediction algorithm will fail. Nonetheless, in many cases where the prediction is sufficiently accurate that the user will get the sense that the application functions at some rudimentary level of success.  Finally, I observed that the distribution of n-grams is different across the three source files. 

I took inspiration from the works of Michael Brown.

